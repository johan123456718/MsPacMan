{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "australian-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pylab\n",
    "import gym\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exposed-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        if load_model:\n",
    "            self.state_size = state_size #Get size of the state\n",
    "            self.action_size = action_size #Get size of the action\n",
    "\n",
    "            #Hyperparameters\n",
    "            self.discount_factor = 0.99 #Disocunt Factor\n",
    "            self.learning_rate = 0.000001 #Learning Rate\n",
    "\n",
    "            #Hyperparameters to adjust the Exploitation-Explore tradeoff\n",
    "            self.epsilon = 0.2 #Setting the epislon (0= Explore, 1= Exploit)\n",
    "            self.epsilon_decay = 0.999999 #Adjusting how our epsilon will decay\n",
    "            self.epsilon_min = 0.2 #Min Epsilon\n",
    "\n",
    "            self.batch_size = 64 #Batch Size for training the neural network\n",
    "            self.train_start = 1000 #If Agent's memory is less, no training is done\n",
    "\n",
    "        else:\n",
    "            self.state_size = state_size #Get size of the state\n",
    "            self.action_size = action_size #Get size of the action\n",
    "\n",
    "            #Hyperparameters\n",
    "            self.discount_factor = 0.99 #Disocunt Factor\n",
    "            self.learning_rate = 0.001 #Learning Rate\n",
    "\n",
    "            #Hyperparameters to adjust the Exploitation-Explore tradeoff\n",
    "            self.epsilon = 1.0 #Setting the epislon (0= Explore, 1= Exploit)\n",
    "            self.epsilon_decay = 0.999 #Adjusting how our epsilon will decay\n",
    "            self.epsilon_min = 0.1 #Min Epsilon\n",
    "\n",
    "            self.batch_size = 64 #Batch Size for training the neural network\n",
    "            self.train_start = 1000 #If Agent's memory is less, no training is done\n",
    "            \n",
    "            \n",
    "        # create main replay memory for the agent using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        #Loading weights if load_model=True\n",
    "        if load_model:\n",
    "            self.model.load_weights(\"./pacman.h5\")\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))#State is input\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))#Q_Value of each action is Output\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    # save sample <state,action,reward,nest_state> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-metro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 50,697\n",
      "Trainable params: 50,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-masters",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 180.0   memory length: 538   epsilon: 0.5837577583990794\n",
      "episode: 1   score: 140.0   memory length: 1119   epsilon: 0.32642343068557345\n",
      "episode: 2   score: 1430.0   memory length: 2000   epsilon: 0.10925100584600911\n",
      "episode: 3   score: 2150.0   memory length: 2000   epsilon: 0.09994334856146549\n",
      "episode: 4   score: 380.0   memory length: 2000   epsilon: 0.09994334856146549\n",
      "episode: 5   score: 230.0   memory length: 2000   epsilon: 0.09994334856146549\n",
      "episode: 6   score: 270.0   memory length: 2000   epsilon: 0.09994334856146549\n",
      "episode: 7   score: 450.0   memory length: 2000   epsilon: 0.09994334856146549\n",
      "episode: 8   score: 1090.0   memory length: 2000   epsilon: 0.09994334856146549\n"
     ]
    }
   ],
   "source": [
    "EPISODES   = 21\n",
    "\n",
    "for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done:\n",
    "            dead = False\n",
    "            while not dead:\n",
    "                env.render()\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # When Pacman dies gives penalty of -100\n",
    "                reward = reward if not dead else -100\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig( \"pacmanTest.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                         len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "        if (e % 20 == 0) & (load_model==False):\n",
    "            agent.model.save_weights(\"pacman.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-equivalent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
